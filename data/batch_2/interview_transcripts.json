[
  {
    "interview_id": "INT-041",
    "date": "December 30, 2024",
    "participant_id": "P-068",
    "participant_role": "Engineering Manager",
    "participant_company_size": "501-1000",
    "interviewer": "David Kim",
    "interview_type": "discovry",
    "product_areas_discussed": [
      "iOS",
      "notifications"
    ],
    "transcript": "[00:00:39] David Kim: Tell me about a time when you were offline and needed to access information on your device.\n[00:02:51] Participant: I was on a flight to Chicago and needed to review some account details before my meeting right after landing. The app has no offline mode whatsoever. I opened it and got a full-screen error saying 'No internet connection' with no option to view cached data. I had previously viewed those exact accounts on the app, so the data was literally on my phone at some point, but the app doesn't store anything locally. I had to rely on some screenshots I'd taken before the flight, which was lucky. If I hadn't planned ahead, I would have walked into that meeting completely unprepared.\n\n[00:04:04] David Kim: Walk me through how the phone experience compares to what you're used to on your computer.\n[00:07:52] Participant: It feels like two completely different products built by two different teams. The terminology is sometimes different. On desktop a section is called 'Analytics' but on the phone it's labeled 'Insights' for some reason. Some features that exist on desktop are just completely missing on the phone, like the ability to create custom views. Other things exist on the phone but work differently, like the filtering which uses a different set of operators. I have to maintain two mental models. My team jokes that we need a training session specifically for the phone version because knowing the desktop doesn't transfer.\n\n[00:11:10] David Kim: Walk me through how you currently manage which alerts you receive and through which channels.\n[00:13:54] Participant: It's a mess honestly. There are three different places where notifications are configured. There's a global setting under my profile, a per-project setting inside each project, and then individual modules have their own alert toggles. These three layers don't always agree. I once turned off email alerts globally but was still getting them because the project-level setting overrode my global preference. There's no single view that shows me all my active alert subscriptions across everything.\n\n[00:16:01] David Kim: Tell me about using the camera or device features through the app.\n[00:17:29] Participant: We were supposed to be able to scan business cards at a conference and have the contact info automatically populate. I tried it with about fifteen cards. The scan worked on maybe six of them. The rest either couldn't be recognized or the OCR parsed the fields wrong. Job titles ended up in the phone number field, email addresses were split across two fields. For the ones that did work, the created contacts didn't have any link back to the scanned image, so I couldn't verify the information later. I ended up just taking photos and manually entering everything, which is exactly what I was doing before the feature existed.\n\n[00:19:32] David Kim: Can you describe a specific situation where you were overwhelmed by too many automated messages?\n[00:21:52] Participant: After we launched a new feature, I started getting an email for every single user action within that feature. I'm talking 300 emails a day. There was no digest option, no frequency cap. I tried to turn off just the activity emails but the only toggle was all-or-nothing for the entire feature module. If I turned it off, I'd also lose critical error alerts. So for two weeks I was getting 300 emails a day until the engineering team pushed an update to add more granular controls. My inbox is still recovering.\n\n[00:27:54] David Kim: Tell me about how your team coordinates around making sure the right people hear about the right events.\n[00:28:06] Participant: We've built this elaborate routing system using Slack channels. There's an ops-alerts channel, an eng-critical channel, a sales-notifications channel, and so on. Each one subscribes to different event types. But the platform itself doesn't support channel-based routing natively. We use a webhook that sends everything to a middleware service I wrote in Python that then routes to the appropriate Slack channel based on event type. If my middleware goes down, our entire notification routing breaks. I maintain it on my personal time which is not sustainable.\n\n[00:30:26] David Kim: Can you describe the most frustrating notification experience you've had with any software tool?\n[00:31:53] Participant: The push notifications on this platform have no concept of priority. A comment on a minor task and a critical server alert look exactly the same on my phone. Same sound, same banner style, same position. I started ignoring all push notifications because 95% were low-priority, and then I missed a genuine critical incident because it looked identical to the twenty comment notifications I'd already dismissed that day. [audio cuts out] I now have a separate Slack bot that only forwards critical alerts, which is a workaround I shouldn't need.\n\n[00:34:42] David Kim: Tell me about how syncing works when you switch between your phone and your laptop.\n[00:37:42] Participant: The sync is not real-time and it's not consistent. I've made edits on my phone, switched to my laptop, and the changes weren't there. Sometimes they show up in five minutes, sometimes thirty. Twice I've had a situation where I edited a record on my phone and then opened it on desktop and made more edits, and the phone changes overwrote my desktop changes when they finally synced. There's no conflict resolution. Last edit wins, regardless of which was more complete. I've lost work to this at least three times.\n",
    "duration_minutes": 44,
    "recording_quality": "good"
  },
  {
    "interview_id": "INT-042",
    "date": "November 13, 2024",
    "participant_id": "P-067",
    "participant_role": "Marketing Manager",
    "participant_company_size": "1000+",
    "interviewer": "Laura Martinez",
    "interview_type": "behavioral",
    "product_areas_discussed": [],
    "transcript": "[00:00:59] Laura Martinez: Can you describe a specific situation where you were confused about what you were being billed for?\n[00:01:32] Participant: Every single invoice we get has these cryptic line item codes like 'ENT-PRO-M-23-Q4' and I have no idea what any of them mean. There's no legend or breakdown. When I asked support, they said it stands for Enterprise Pro Monthly seat count for Q4 of 2023, but there's six other line items I still can't decode. I've been asking for a plain-English invoice for months. My finance team has flagged it as an audit risk because we can't reconcile what we're paying for.\n\n[00:06:43] Laura Martinez: Walk me through what happens when your CRM data needs to appear in your project management tool.\n[00:09:17] Participant: It's basically a Rube Goldberg machine at this point. A rep closes a deal in Salesforce, that triggers a Zapier webhook which creates a project in Asana, and then a separate Zapier zap copies the account details into a Google Sheet that our ops team uses. If any one of those steps fails, everything downstream is broken. Last month the Zapier webhook failed silently for about 48 hours and we had twelve new customers with no onboarding projects created.\n\n[00:12:38] Laura Martinez: Tell me about a time when you needed to update your payment method or switch billing contacts.\n[00:15:27] Participant: When our CFO changed, I needed to update the billing email and the credit card on file. Updating the email was simple enough, but changing the payment card required re-entering our full company address, tax ID, and re-verifying our organization. The verification process took 48 hours during which our account was in a limited state and my team couldn't access premium features. That was completely unacceptable for a payment method change.\n\n[00:21:57] Laura Martinez: Walk me through how you typically handle upgrading or downgrading your team's subscription.\n[00:24:09] Participant: It's actually pretty nerve-wracking because there's no preview of what the prorated amount will be. I have to just click upgrade and then find out what we're being charged. Last time we went from the Team plan to the Business plan mid-cycle, and the prorated amount was way higher than I expected because it charged us retroactively for the full month on all existing seats. Our finance controller was not happy about the surprise charge on the corporate card.\n\n[00:29:46] Laura Martinez: Tell me about how your team decided which external tools to wire up and the process of getting them working.\n[00:31:37] Participant: We did a big audit about six months ago where we listed every tool we use and how data moves between them. We identified fourteen different tools and realized only four had native connectors. For the rest we were using a combination of Zapier, custom scripts, and honestly just copy-paste. Prioritizing which to automate first was hard because every team thought their workflow was the most critical. We ended up spending about $40K on middleware licensing alone.\n\n[00:36:42] Laura Martinez: Walk me through how data currently flows between the different systems your team uses.\n[00:39:03] Participant: So we have Salesforce as our source of truth for customer records, then that pushes into HubSpot for marketing automation, and separately into our internal tool for account management. The Salesforce-to-HubSpot sync runs every fifteen minutes but the internal tool sync is only hourly. The problem is that when a sales rep updates a contact in Salesforce, the marketing team might be working with stale data for up to an hour. We've had campaigns go out to people who were already marked as churned.\n",
    "duration_minutes": 66,
    "recording_quality": "fair"
  },
  {
    "interview_id": "INT-043",
    "date": "12/25/2024",
    "participant_id": "P-082",
    "participant_role": "CTO/Co-founder",
    "participant_company_size": "medium-large",
    "interviewer": "David Kim",
    "interview_type": "usabilty",
    "product_areas_discussed": [],
    "transcript": "Q: Tell me about a time when the developer documentation didn't...\nuser creation endpoint - docs say role field accepts string - actually only numeric role ID - string doesn't error just sets role to null - no permissions - 2hrs debugging broken provisioning - support said docs outdated\n\nQ: Can you describe the most frustrating experience you've had ...\nhistorical event data 90 days - endpoint default 30 days - docs wrong start_date/end_date actually from/to - response format different >30 days vs recent - different field names different nesting - two separate parsers same data\n\nQ: Can you describe a specific situation where you were confuse...\ncryptic line item codes ENT-PRO-M-23-Q4 - no legend or breakdown - support decoded one but 6 others unknown - asked for plain English invoice months - finance flagged audit risk - can't reconcile\n\nQ: Can you describe a time when you received a charge you thoug...\ncancelled advanced data processing September - got confirmation email - still charged $350/month November - support said annual add-on contract cancellation at renewal - not in cancellation flow - paid 5 extra months unused\n\nQ: Walk me through the last time you tried to customize a chart...\nfunnel chart for sales leadership - 2 weeks ago - couldn't rename stage labels - pulled cryptic DB field names - wanted benchmark line - only works with [inaudible] bar charts not funnels\n\nQ: Tell me about how you share periodic summaries with leadersh...\nmanual process - generate summary export PDF - paste highlights Slack - VP wants email attachment separately - wants auto-send Slack + email DL - save 45 min/week\n",
    "duration_minutes": 65,
    "recording_quality": "poor"
  },
  {
    "interview_id": "INT-044",
    "date": "2024-10-11T11:33:58",
    "participant_id": "P-053",
    "participant_role": "Analytics Lead",
    "participant_company_size": "medium-large",
    "interviewer": "Maria Santos",
    "interview_type": "discovery",
    "product_areas_discussed": [],
    "transcript": "Q: Can you describe the validation feedback you get when an upl...\nerror messages useless - 'Row 847 validation error' no field or detail - 200 rows with errors - manually check each row against undocumented validation rules - row 847 zip code leading zero stripped by Excel - 45min per error type to discover\n\nQ: Walk me through how your organization manages single sign-on...\nOkta SSO - SCIM provisioning flaky since day one - users created but roles don't sync - everyone gets default permissions - manual role config each person - SCIM role mapping 'on roadmap' 8 months ago - 300 people regular turnover - real burden\n\nQ: Walk me through how you currently handle setting up permissi...\n60+ individual permission toggles - not grouped intuitively - read vs export vs share all separate - 2hrs creating read-only analyst role - not confident about accidental delete perms - no preview or test mode\n\nQ: Walk me through your typical process for preparing a spreads...\nritual - match column headers exactly trailing space breaks mapping - check special chars accented characters encoding issues - remove empty rows at bottom Excel artifacts - convert all dates ISO format only format accepted but only told after upload starts - prep longer than upload\n",
    "duration_minutes": 59,
    "recording_quality": "fair"
  },
  {
    "interview_id": "INT-045",
    "date": "11/14/2024",
    "participant_id": "P-035",
    "participant_role": "Sales Director",
    "participant_company_size": "1-50",
    "interviewer": "Laura Martinez",
    "interview_type": "feedback",
    "product_areas_discussed": [
      "data-import"
    ],
    "transcript": "Interviewer: Can kind of you kind of describe the actually validation feedback you get when an uh upload uh has errors?\nParticipant: The error messages are almost useless. I'll get something like kind of 'Row 847: validation right error' um with no detail about sort of which field or basically what the error is. Last time I had 200 rows with yeah errors and the only way to find right the problems was to open the original like CSV, go to row 847 and actually manually basically check each field actually against the system's validation honestly rules Which aren't documented like anywhere by the way I eventually figured out that row 847 had like a zip code sort of with a leading zero that got sort of stripped right by Excel But discovering that took uh me kind of forty-five minutes per error type \n\nInterviewer: Tell me about handling files that have merged cells or inconsistent row I mean formatting \nParticipant: Our actually finance team sends me these Excel files where actually they've merged cells across rows for grouping purposes The like upload parser honestly treats merged cells as one record with blank fields for the rows below. So a merged company like name sort of cell spanning five rows results in sort of one uh complete record and four so records with basically empty company names I've told the kind of finance team to stop merging honestly cells but they say their spreadsheet is also used for printing I mean and the merged cells look sort of better. Now I run kind of an unmerge macro before actually every upload.\n\nInterviewer: Can you describe the most painful experience you've had yeah trying to so get external data into right the platform?\nParticipant: We were migrating from a legacy system like that exported data in a you know fixed-width text format uh not CSV. The platform only accepts CSV XLSX, or JSON. So kind of I had too um write a you know Python um script honestly to convert the fixed-width um format to CSV. But the legacy system used uh Windows-1252 encoding basically and the platform only supports UTF-8 After converting the sort of encoding, about 400 customer names with non-English so characters were garbled Names like Muller you know with an umlaut became 'M\\u00fcller' in I mean the database. We had to um manually fix basically each one because there um was no way to identify all affected records programmatically.\n\nInterviewer: Tell me about how you deal you know with files that have special sort of characters or unusual encoding.\nParticipant: I've learned the right hard way to always kind of open the uh file honestly in a text editor first and check um the encoding um before uploading We work with a lot of Latin American right clients so names with I mean tildes accent uh marks, actually and the letter uh enye are common The platform's like upload preview shows them um correctly but after upload they're corrupted in sort of the database I now run every file kind of through so a UTF-8 kind of conversion honestly script actually before uploading, but that shouldn't be my job. right The platform should handle common encodings gracefully. This right has cost us at honestly least twenty hours basically of sort of cleanup time over the last so quarter.\n\nInterviewer: Tell me about a so time when the column kind of headers um in uh your file didn't map right correctly to the system sort of fields.\nParticipant: Every time we get data from a new partner the column names are honestly different. kind of One calls it right 'Company Name', another uses 'Organization', another uses so 'org_name' so The auto-mapping tries to match I mean them but it gets confused easily Last week it mapped 'Phone I mean Number' to the 'Fax' field because that was the closest match I mean it could find I didn't catch it until after um the upload right completed and now we have 3 000 basically records kind of with um phone numbers in the fax right field The manual kind of mapping you know interface uh requires clicking each field individually from a yeah dropdown of over 200 honestly system actually fields.\n\nInterviewer: Can you describe a specific like situation where a file so upload you know failed or produced duplicate records?\nParticipant: We had a vendor send us a right CSV right with about 8,000 product records Uploaded it looked great. you know Then someone noticed basically our total product count jumped from 15 000 to I mean 23 000 instead of staying at 15,000 with updated sort of records. The system you know created new records instead of matching on the product SKU and so updating. Turns basically out the match key setting defaults to 'create new' sort of instead of 'match and update' and it's buried three you know clicks deep in the upload settings. We had right to manually delete 8,000 duplicate like records which took you know our admin the basically entire uh afternoon \n\nInterviewer: Walk me through how you handle cleaning uh up basically data [audio cuts out] after a bulk actually upload goes partially wrong.\nParticipant: It's painful. There's no undo or like rollback for uploads. If um something goes wrong, you have um to identify so which records I mean were affected and kind of the right system yeah doesn't tag honestly records by you know upload batch. So yeah I have to sort of filter by creation date and time, hoping nothing else was created during that kind of window Then I either fix honestly the sort of bad records one by one or delete them all and basically re-upload. The kind of mass delete sort of operation has a limit uh of 500 honestly records at um a you know time so for our 8,000 duplicate situation I mean I I mean had to run the honestly delete sixteen times.\n\nInterviewer: Walk me through a so time when yeah you needed to upload more than ten um thousand rows at once.\nParticipant: We sort of had a backlog of 60 000 transaction actually records that needed to be imported from a honestly spreadsheet The system has you know a 10,000 row limit basically per upload, uh so I had right to um split the file into six separate CSVs But the system also has a sort of per-hour upload limit kind of of 20,000 records so I could only do two files you know per hour. The entire process you know took like over three hours of like me just sitting there waiting and uploading the next batch. sort of And each upload required me to redo the column mapping because you know it doesn't save your mapping between uploads. That's another five minutes per file.\n",
    "duration_minutes": 33,
    "recording_quality": "good"
  },
  {
    "interview_id": "INT-046",
    "date": "2024-10-26T19:06:51",
    "participant_id": "P-105",
    "participant_role": "Customer Success Manager",
    "participant_company_size": "1000+",
    "interviewer": "James Chen",
    "interview_type": "behavioral",
    "product_areas_discussed": [],
    "transcript": "Q: Walk me through what you do when the connection between two ...\npanic - no good monitoring on connectors - someone notices stale data - scramble to diagnose - expired credential or rate limit or schema change - 4hrs to diagnose - error logs just 'sync failed' no details\n\nQ: Tell me about a time when you had to set up an authorization...\nSSO provider SAML 2.0 setup - attribute mapping undocumented - 3 days with support - group claims not passing - case sensitivity issue in attribute names\n\nQ: Walk me through the last time you added or removed a compone...\nadd churn prediction widget - 50+ widget catalog no search no categories - alphabetical scroll list - found under 'Predictive Churn Indicator' not customer - added at very bottom below fold - drag past 12 widgets each drag full re-render - 5 min for 10 sec task\n\nQ: Tell me about how different people on your team configure th...\nno overlap in team landing pages - PM product metrics - eng lead system health + deploys - designer user feedback - meetings all different versions of reality - no shared team view - screenshare or exported snapshot for alignment\n",
    "duration_minutes": 72,
    "recording_quality": "poor"
  },
  {
    "interview_id": "INT-047",
    "date": "2024-12-17T10:20:02",
    "participant_id": "P-088",
    "participant_role": "product manager",
    "participant_company_size": "51-200",
    "interviewer": "Maria Santos",
    "interview_type": "feedback",
    "product_areas_discussed": [
      "data-import",
      "lookup"
    ],
    "transcript": "[00:00:29] Maria Santos: Tell me about how you deal with files that have special characters or unusual encoding.\n[00:01:07] Participant: I've learned the hard way to always open the file in a text editor first and check the encoding before uploading. We work with a lot of Latin American clients so names with tildes, accent marks, and the letter enye are common. The platform's upload preview shows them correctly but after upload they're corrupted in the database. I now run every file through a UTF-8 conversion script before uploading, but that shouldn't be my job. The platform should handle common encodings gracefully. This has cost us at least twenty hours of cleanup time over the last quarter.\n\n[00:05:58] Maria Santos: Tell me about how your team handles finding records that were created by someone else.\n[00:06:41] Participant: This comes up constantly with our sales team. One rep needs to look at an account that another rep owns. There's no 'created by' or 'owned by' filter in the main lookup. You have to either know the exact account name, or ask the owner to send you a direct link. We've had situations where two reps were working the same account because neither could easily see it was already claimed. We actually added a shared Notion page where reps log which accounts they're working on, which is a ridiculous workaround for a platform that should have basic ownership visibility in its lookup.\n\n[00:08:59] Maria Santos: Can you describe the validation feedback you get when an upload has errors?\n[00:11:44] Participant: The error messages are almost useless. I'll get something like 'Row 847: validation error' with no detail about which field or what the error is. Last time I had 200 rows with errors and the only way to find the problems was to open the original CSV, go to row 847, and manually check each field against the system's validation rules. Which aren't documented anywhere by the way. I eventually figured out that row 847 had a zip code with a leading zero that got stripped by Excel. But discovering that took me forty-five minutes per error type.\n\n[00:15:52] Maria Santos: Can you describe the most frustrating experience you've had trying to locate information in the system?\n[00:16:52] Participant: I was trying to find a specific support conversation we had with a customer about a billing dispute from about three months ago. I knew the customer name and roughly when it happened. The system doesn't index conversation content for the lookup, so searching the customer name only showed me their account record, not the conversation. I had to go into the account, find the activity timeline, scroll through three months of entries because there's no date range filter on the timeline, and visually scan for the conversation. It took me twenty-five minutes. An intern could have done it faster with Ctrl+F on a spreadsheet.\n",
    "duration_minutes": 26,
    "recording_quality": "good"
  },
  {
    "interview_id": "INT-048",
    "date": "December 14, 2024",
    "participant_id": "P-068",
    "participant_role": "Operations Manager",
    "participant_company_size": "1-50",
    "interviewer": "Aisha Patel",
    "interview_type": "usability",
    "product_areas_discussed": [],
    "transcript": "Interviewer: Can you describe the most painful handoff between two systems you deal with regularly?\nParticipant: The handoff between our ticketing system and our engineering sprint board is brutal. When a customer-facing bug is filed in Zendesk, someone on our CS team has to manually create a corresponding ticket in Jira, copy over all the context, attach the screenshots, and tag the right engineering team. There's no automatic linking. So when engineering fixes the bug, CS has no idea unless someone remembers to go back and update the Zendesk ticket. Things fall through the cracks constantly.\n\nInterviewer: Walk me through a time when a contractor or external partner needed limited access to your system.\nParticipant: We brought in a consulting firm to help with our data migration and they needed access to view our data structures but absolutely not modify anything. The platform doesn't have a concept of external or guest users with time-limited access. I created regular accounts for the five consultants and tried to lock them down to read-only. But read-only still let them export data, which the consultants shouldn't have been able to do. I ended up having to shadow their sessions for the first week until I was satisfied they weren't exporting anything sensitive.\n\nInterviewer: Walk me through how data currently flows between the different systems your team uses.\nParticipant: So we have Salesforce as our source of truth for customer records, then that pushes into HubSpot for marketing automation, and separately into our internal tool for account management. The Salesforce-to-HubSpot sync runs every fifteen minutes but the internal tool sync is only hourly. The problem is that when a sales rep updates a contact in Salesforce, the marketing team might be working with stale data for up to an hour. We've had campaigns go out to people who were already marked as churned.\n\nInterviewer: Can you describe a situation where you needed real-time data exchange between two platforms but had to settle for batch?\nParticipant: We really need our inventory system and our e-commerce platform to be in real-time sync. Right now it only updates every thirty minutes, and during flash sales we've oversold products because the stock count was stale. Last Black Friday we oversold 200 units of a popular item because the batch sync hadn't caught up. Customer service was dealing with refund requests for two weeks after that. We've been asking for real-time webhooks for over a year.\n\nInterviewer: Walk me through how you currently handle setting up permissions for different roles on your team.\nParticipant: It's incredibly granular which sounds like a good thing but in practice it's paralyzing. There are over sixty individual permission toggles and they're not grouped in any intuitive way. Read access to the analytics module is separate from export access which is separate from share access. I spent two hours last week trying to create a 'read-only analyst' role and I'm still not confident I didn't accidentally give them delete permissions on something. There's no preview or test mode.\n\nInterviewer: Tell me about a time when you had to set up an authorization flow to get two platforms talking to each other.\nParticipant: Setting up our SSO provider to work with the platform was genuinely painful. The documentation said it supported SAML 2.0, which it did, but the attribute mapping was completely undocumented. We spent three days going back and forth with support trying to figure out why the group claims weren't being passed through correctly. It turned out there was a case-sensitivity issue in one of the attribute names that nobody mentioned anywhere.\n\nInterviewer: Tell me about a time when you had to deal with offboarding someone and revoking all their access.\nParticipant: When our head of sales left, we needed to revoke his access immediately because he was going to a competitor. But there's no single 'deactivate this person everywhere' button. I had to remove him from each team individually, revoke his API credentials separately, unshare every shared view he had access to, and then deactivate his actual account. The whole process took about forty-five minutes and the entire time I was worried he still had access to something. We really need a one-click offboarding workflow.\n\nInterviewer: Can you describe the most complicated permissions setup you've had to configure?\nParticipant: We have a client services team that needs to see customer data for only their assigned accounts. Not all accounts, just theirs. The system doesn't support row-level access based on account ownership, so I had to create a separate team for every major client and manually assign the relevant CS reps to each team. We have forty-two enterprise clients. That's forty-two teams I have to maintain. Every time a CS rep changes assignments, I have to move them between teams manually. It's unsustainable.\n",
    "duration_minutes": 38,
    "recording_quality": "good"
  },
  {
    "interview_id": "INT-049",
    "date": "",
    "participant_id": "P-014",
    "participant_role": "Product Designer",
    "participant_company_size": "51-200",
    "interviewer": "James Chen",
    "interview_type": "discovery",
    "product_areas_discussed": [],
    "transcript": "Q: Walk me through a situation where battery drain from the app...\ntwo-day conference - 40% battery by lunch only 20min use - constant background processes - turned off background refresh helped but push notifications stopped - choose between alerts or battery lasting - carrying power bank specifically for this app\n\nQ: Tell me about a time when you needed to update your payment ...\nCFO changed - update billing email easy - change card required full re-verification - company address tax ID org verification - 48hr limited state - no premium features - unacceptable for payment change\n\nQ: Tell me about how your finance team handles reconciling soft...\nmonthly headache - cross-reference invoice with headcount spreadsheet - dates never aligned always discrepancy - no individual user list just total seats - over by 2 seats can't identify which - 3 weeks to close books last quarter - 20+ SaaS subs\n\nQ: Tell me about a time when you were offline and needed to acc...\nflight to Chicago - review account details before meeting - no offline mode at all - full-screen error 'No internet connection' - no cached data option - had previously viewed accounts data was on phone - doesn't store locally - relied on screenshots taken before flight - lucky had planned ahead\n\nQ: Tell me about the last time you had a question about a charg...\ncharged $2400 expected $1800 - overage charges additional seats - hadn't added new members - guest accounts count as seats - nobody knew - 3 emails 5 days to get credit\n\nQ: Walk me through what you typically try to accomplish on the ...\nphone for quick checks approvals - look at metric approve request reply to comment - same nested menu structure as desktop - 4 levels to get to approvals - no quick actions or shortcuts for common mobile tasks - suggested 'mobile home' view top 5 actions - nothing so far\n",
    "duration_minutes": 29,
    "recording_quality": "poor"
  },
  {
    "interview_id": "INT-050",
    "date": "2024-10-05T08:56:03",
    "participant_id": "P-010",
    "participant_role": "eng. lead",
    "participant_company_size": "",
    "interviewer": "Laura Martinez",
    "interview_type": "discovery",
    "product_areas_discussed": [],
    "transcript": "Interviewer: Walk me through a time when a breaking change in an external service caused issues for your application.\nParticipant: They released a v3 of their endpoint in October and deprecated v2 with a 60-day sunset. The problem is the v3 response payload was completely restructured, and our entire data pipeline was built around the v2 schema. It wasn't just field renames - they changed the data model fundamentally. What was a flat list became a nested tree structure. We had to rewrite our ETL pipeline, which took our team of four engineers about three weeks. And during the migration [background noise] period we had to maintain both v2 and v3 compatibility.\n\nInterviewer: Walk me through how you test your calls before deploying them to production.\nParticipant: There's no sandbox environment, which is a real problem. We test against our production instance with a test account. But there's no way to flag requests as test requests, so our analytics get polluted with test data. I've asked about a staging environment or at minimum a test mode flag on requests, and the response was that it's not a priority. So we end up writing cleanup scripts that delete test data after our testing runs, which itself creates more calls against our already tight rate limits.\n\nInterviewer: Walk me through what happens when you get an email from the system - do you usually act on it or ignore it?\nParticipant: I'd say I act on maybe 10% of the emails I get from the platform. The problem is signal-to-noise ratio. I get about forty system emails a day and maybe four of them require action. The subject lines are all the same format so I can't visually scan for important ones. They all say something like 'Notification from Platform - Event Update.' No severity level, no category, no preview of what happened. I've set up Gmail filters to try to sort them but the email format doesn't give me enough metadata to filter effectively.\n\nInterviewer: Can you describe the most frustrating notification experience you've had with any software tool?\nParticipant: The push notifications on this platform have no concept of priority. A comment on a minor task and a critical server alert look exactly the same on my phone. Same sound, same banner style, same position. I started ignoring all push notifications because 95% were low-priority, and then I missed a genuine critical incident because it looked identical to the twenty comment notifications I'd already dismissed that day. I now have a separate Slack bot that only forwards critical alerts, which is a workaround I shouldn't need.\n\nInterviewer: Can you describe the most frustrating experience you've had trying to get data programmatically?\nParticipant: Trying to get historical event data for the last 90 days was a nightmare. The events endpoint only returns the last 30 days by default, and the documentation for the date range parameters was just wrong. It said to use 'start_date' and 'end_date' but the actual parameter names were 'from' and 'to'. Then when I got the right parameters, the response format for events older than 30 days was completely different from recent events. Different field names, different nesting structure. I had to write two separate parsers for what should be the same data.\n\nInterviewer: Tell me about a time when the developer documentation didn't match the actual behavior of an endpoint.\nParticipant: This happens all the time honestly. The most recent example was the user creation endpoint. The docs said the 'role' field accepts a string like 'admin' or 'viewer'. But in practice it only accepts a numeric role ID. When I passed the string, it didn't error out - it just silently set the role to null and the user was created with no permissions. I spent two hours debugging why my provisioning script was creating broken accounts before a support engineer told me the docs were outdated.\n\nInterviewer: Can you describe a specific situation where you were overwhelmed by too many automated messages?\nParticipant: After we launched a new feature, I started getting an email for every single user action within that feature. I'm talking 300 emails a day. There was no digest option, no frequency cap. I tried to turn off just the activity emails but the only toggle was all-or-nothing for the entire feature module. If I turned it off, I'd also lose critical error alerts. So for two weeks I was getting 300 emails a day until the engineering team pushed an update to add more granular controls. My inbox is still recovering.\n\nInterviewer: Tell me about the last time you missed an important alert that you should have received.\nParticipant: ...I had to stay late, ni modo, because the deadline was the next day...\nParticipant: Two weeks ago our production database hit 95% capacity. There was supposedly an alert configured for that threshold, but I never got it. When I dug into it, the alert had been sent to an email distribution list that was decommissioned six months ago as part of an email migration. The system showed it as 'delivered' because the email server accepted it, even though the mailbox no longer existed. Nobody was monitoring that dead distribution list. We caught the issue only because a developer happened to notice slow queries.\n",
    "duration_minutes": 34,
    "recording_quality": "fair"
  },
  {
    "interview_id": "INT-051",
    "date": "2024-10-12T18:04:12",
    "participant_id": "P-004",
    "participant_role": "Chief Technology Officer",
    "participant_company_size": "1000+",
    "interviewer": "Aisha Patel",
    "interview_type": "behaviorial",
    "product_areas_discussed": [
      "payments"
    ],
    "transcript": "Q: Tell me about a time when you needed to update your payment ...\nCFO changed - update billing email easy - change card required full re-verification - company address tax ID org verification - 48hr limited state - no premium features - unacceptable for payment change\n\nQ: Tell me about how your finance team handles reconciling soft...\nmonthly headache - cross-reference invoice with headcount spreadsheet - dates never aligned always discrepancy - no individual user list just total seats - over by 2 seats can't identify which - 3 weeks to close books last quarter - 20+ SaaS subs\n\nQ: Can you describe a specific situation where you were confuse...\ncryptic line item codes ENT-PRO-M-23-Q4 - no legend or breakdown - support decoded one but 6 others unknown - asked for plain English invoice months - finance flagged audit risk - can't reconcile\n\nQ: Walk me through the last time you tried to understand the di...\n\n[transcript incomplete â€” recording ended early]",
    "duration_minutes": 58,
    "recording_quality": "fair"
  },
  {
    "interview_id": "INT-052",
    "date": "2024-12-05T11:30:54",
    "participant_id": "P-011",
    "participant_role": "Sales Director",
    "participant_company_size": "1000+",
    "interviewer": "Laura Martinez",
    "interview_type": "discovery",
    "product_areas_discussed": [],
    "transcript": "Q: Tell me about a time when a critical system event happened a...\npayment processing down 4hrs Saturday - no alerts because alerting system on same infra - single point of failure - customer tweeted about failed payments - social media manager saw - $30K missed transactions - set up third-party monitoring for alerting failures\n\nQ: Can you describe the validation feedback you get when an upl...\nerror messages useless - 'Row 847 validation error' no field or detail - 200 rows with errors - manually check each row against undocumented validation rules - row 847 zip code leading zero stripped by Excel - 45min per error type to discover\n\nQ: Can you describe a time when alert fatigue led to a real con...\nCI/CD pipeline flaky test 12 failures/day - team started ignoring pipeline emails after a week - legitimate deployment failure data corruption bug - nobody caught from alerts desensitized - bug in production 3 days customer reported - 2000 records affected - root cause couldn't distinguish important from noise\n\nQ: Walk me through how you handle cleaning up data after a bulk...\nno undo or rollback - no upload batch tagging - filter by creation date/time hoping nothing else created - fix one by one or delete all re-upload - mass delete limit 500 records - 8K duplicates = 16 delete operations\n\nQ: Walk me through how you set up rules for when and how you wa...\nrule builder simple surface - trigger event + condition + channel - conditions limited to exact match - can't do % comparisons to previous periods - static threshold only for dynamic metrics - constantly adjusting as we grow - can't combine conditions AND logic - each rule independent\n\nQ: Walk me through your typical process for preparing a spreads...\nritual - match column headers exactly trailing space breaks mapping - check special chars accented characters encoding issues - remove empty rows at bottom Excel artifacts - convert all dates ISO format only format accepted but only told after upload starts - prep longer than upload\n",
    "duration_minutes": 59,
    "recording_quality": "fair"
  }
]